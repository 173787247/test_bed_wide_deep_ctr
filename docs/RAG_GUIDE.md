# RAG问答系统使用指南

## 概述

RAG（Retrieval-Augmented Generation）问答系统基于现有的倒排索引和TF-IDF算法，结合Ollama大语言模型，实现检索增强生成功能。

## 功能特点

### 🔍 检索功能
- 基于现有的倒排索引进行快速文档检索
- 使用TF-IDF算法计算文档相关度分数
- 支持调整检索文档数量（1-10个）

### 🤖 生成功能
- 集成Ollama本地大语言模型
- 支持多种模型选择（llama3.1:8b、qwen2.5:7b等）
- 基于检索到的上下文生成准确回答

### 📊 透明度
- 显示检索到的文档详情
- 展示完整的上下文信息
- **显示发送给LLM的完整提示词**

## 使用方法

### 1. 系统要求
- 确保Ollama服务正在运行（默认端口：11434）
- 已安装所需模型（如llama3.1:8b）

### 2. 连接检查
点击"🔍 检查Ollama连接"按钮，确认：
- Ollama服务可访问
- 模型列表正常加载

### 3. 执行查询
1. 在"输入您的问题"文本框中输入问题
2. 调整检索文档数量（可选）
3. 选择要使用的模型（可选）
4. 点击"🚀 RAG查询"执行

### 4. 结果解读

#### 生成回答
- 主要的回答内容
- 处理时间和使用的模型信息

#### 完整提示词
- **显示发送给LLM的完整提示词**
- 包含上下文信息和用户问题
- 便于调试和理解生成过程

#### 检索结果详情
- 相关文档列表（ID、相关度分数、内容摘要）
- 完整的上下文信息

## 技术实现

### 检索流程
1. 使用倒排索引快速匹配相关文档
2. 计算TF-IDF相关度分数
3. 按分数排序返回Top-K文档

### 生成流程
1. 将检索到的文档内容拼接为上下文
2. 构建包含上下文和用户问题的提示词
3. 调用Ollama API生成回答
4. **返回生成的回答和使用的提示词**

### 提示词模板
```
基于以下上下文信息，回答用户的问题。如果上下文中没有相关信息，请说明无法根据提供的信息回答。

上下文信息：
{context}

用户问题：{query}

请用中文回答：
```

## 故障排除

### 常见问题
1. **连接失败**：检查Ollama服务是否运行
2. **模型不可用**：确认模型已正确安装
3. **检索无结果**：检查索引是否正确构建
4. **生成超时**：增加模型响应时间限制

### 性能优化
- 调整检索文档数量以平衡质量和速度
- 选择合适的模型以平衡准确性和响应时间
- 使用提示词透明度功能优化prompt设计

## 扩展功能

### 自定义提示词
系统显示完整的提示词，便于：
- 理解生成过程
- 调试回答质量
- 优化提示词设计

### 多轮对话
当前版本支持单轮问答，未来可扩展：
- 对话历史记录
- 上下文延续
- 多轮推理

## 注意事项

1. 确保Ollama服务在localhost:11434运行
2. 模型下载可能需要较长时间
3. 生成质量依赖于检索到的文档质量
4. 提示词设计影响生成效果 